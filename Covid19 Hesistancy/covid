# -*- coding: utf-8 -*-
"""Data Science Group Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L1rirT0FZf-zBS74uujK42BncsOe9txi
"""

import pandas as pd
import numpy as np

data=pd.read_csv("Covid-19 Hesitancy Form.csv")
data.head(2)

import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

data.shape

data.columns

#Change columns name
data.columns=['time','gender','age','loc','edu_background','vaccinated','dose','vacc1_type','vacc2_type','side_effect1','side_effect2',
           'vacc_type_booster1','vacc_type_booster2','motivation','family_vaccinated','lost_vaccinated','5th_vacc',
           'willing','hesitation','family_unvaccinated','lost_unvaccinated','knowledge','awareness','5th_vacc_unvaccinated',
           'pfizer_rate','reason_pfizer','astrazeneca_rate','reason_astrazeneca','sinovac_rate','reason_sinovac','cansino_rate','reason_cansino',
           'other_vacc','other_vacc_name']

data.head(2)

data.info()

data.age.describe()

"""something is off with max value of age hence we will assume that there is a typing error by replacing the error of 222 to 22


"""

data.age=data.age.replace([222],22)
data.age.describe()

data.age.unique()

data = data.loc[~((data['age'] == 31))]
data.age.unique()

data.vaccinated.unique()

"""**Drop column**


1.   time -> not meaningful
2.   vaccinated -> all rows have same value / only one unique value
3.   willing, hesitation, family_unvaccinated, lost_unvaccinated, knowledge, awareness, 5th_vacc_unvaccinated -> null columns
4.   other_vacc_name -> many missing values


"""

#Drop columns
data=data.drop(['time','vaccinated','willing','hesitation','family_unvaccinated','knowledge','awareness','5th_vacc_unvaccinated','lost_unvaccinated','other_vacc_name'], axis=1)
data.shape

"""now the columns have been reduced from 34 to 24

# Dose preprocess
"""

#Create column with list
#data.assign(dose=data.dose.str.split(";"))

#Now we can use Pandas explode() function to separate list elements into separate rows()
#data=data.assign(dose=data.dose.str.split(";")).explode('dose')

#data.dose.value_counts()

c=round(((data.dose.value_counts()/105)*100),2)
c

data.dose=data.dose.replace(["Booster 1"], "Dose 1;Dose 2;Booster 1")
data.dose=data.dose.replace(["Booster 2"], "Dose 1;Dose 2;Booster 1;Booster 2")
data.dose=data.dose.replace(["Dose 2"], "Dose 1;Dose 2")

round(((data.dose.value_counts()/105)*100),2)

"""# Side effect dose 1 preprocess"""

#a=data.gender.value_counts()/106
#print(a*100)

#a=(data["loc"].value_counts()/106)*100
#a

#a=round(((data.edu_background.value_counts()/106)*100),2)
#a

a=round(((data.side_effect1.value_counts()/105)*100),2)
a

#Repalce data to "None"
data.side_effect1=data.side_effect1.replace(["Nil"],"None").replace("No","None").replace("Alhamdulillah...I'm just fineðŸ¥°","None").replace("Fine","None").replace("No side effects ","None").replace("no side effects","None")

#Replace "Low blood pressure" into common side effect
data.side_effect1=data.side_effect1.replace(["Low blood preasure"], "Common side effects (i.e. fever, vomitting)")

data.side_effect1=data.side_effect1.replace(["Gatal"], "Uncommon side effects (i.e. night sweats, heart inflammation)")

round(((data.side_effect1.value_counts()/105)*100),2)

"""# Side effect Dose 2 Preprocess"""

b=round(((data.side_effect2.value_counts()/105)*100),2)
b

#Replace "Low blood pressure" into common side effect
data.side_effect2=data.side_effect2.replace(["Nil"],"None").replace("No","None").replace("no", "None").replace("Fine","None").replace("No side effects ","None").replace("No side effect","None").replace("no side effects","None")

#Replace "Low blood pressure" into common side effect
data.side_effect2=data.side_effect2.replace(["Low blood preasure"], "Common side effects (i.e. fever, vomitting)")

b=round(((data.side_effect2.value_counts()/105)*100),2)
b

"""# Vaccine type Booster 1 Preprocess"""

data.vacc_type_booster1.value_counts()

#1. replace no and none to 0
#2. handle missing values with 0
data=data.fillna(0)
data.vacc_type_booster1=data.vacc_type_booster1.replace(["No"], 0).replace(["None"], 0)
round(((data.vacc_type_booster1.value_counts()/105)*100),2)

"""# Vaccine_type_booster2 preprocess"""

data.vacc_type_booster2.value_counts()

data.vacc_type_booster2=data.vacc_type_booster2.replace(["None"], 0)
round(((data.vacc_type_booster2.value_counts()/105)*100),2)

"""# motivation preprocess"""

data.motivation.value_counts()

"""From output above we can see that the data contain more than one value in one row. We could not find out the distribution of how frequently the value was appearing without splitting these cells into individual cells of their own which is creating new rows thus we will seperate the data by ; and create new rows."""

#Create column with list
data.assign(motivation=data.motivation.str.split(";"))

#Now we can use Pandas explode() function to separate list elements into separate rows()
data=data.assign(motivation=data.motivation.str.split(";")).explode('motivation')

round(((data.motivation.value_counts()/105)*100),2)

data.info()

"""# reason_pfizer preprocess"""

data.reason_pfizer.value_counts()

#handle outlier by replacing '.' with 'success rate of the vaccine' (why? coz it the highest)
data.reason_pfizer=data.reason_pfizer.replace(["."], "Success rate of the vaccine")

data.reason_pfizer.value_counts()

#Create column with list
data.assign(reason_pfizer=data.reason_pfizer.str.split(";"))

#Now we can use Pandas explode() function to separate list elements into separate rows()
data=data.assign(reason_pfizer=data.reason_pfizer.str.split(";")).explode('reason_pfizer')

data.reason_pfizer.value_counts()

"""# reason_astrazeneca preprocess"""

data.reason_astrazeneca.value_counts()

#handle outlier by replacing '.' with 'success rate of the vaccine' (why? coz it the highest)
data.reason_astrazeneca=data.reason_astrazeneca.replace(["."], "Success rate of the vaccine")

data.reason_astrazeneca.value_counts()

#Create column with list
data.assign(reason_astrazeneca=data.reason_astrazeneca.str.split(";"))

#Now we can use Pandas explode() function to separate list elements into separate rows()
data=data.assign(reason_astrazeneca=data.reason_astrazeneca.str.split(";")).explode('reason_astrazeneca')

data.reason_astrazeneca.value_counts()

"""# reason_sinovac preprocess"""

data["reason_sinovac"].value_counts()

#Create column with list
data.assign(reason_sinovac=data.reason_sinovac.str.split(";"))

#Now we can use Pandas explode() function to separate list elements into separate rows()
data=data.assign(reason_sinovac=data.reason_sinovac.str.split(";")).explode('reason_sinovac')

data.reason_sinovac.value_counts()

#handle outlier by replacing '.' with 'success rate of the vaccine' (why? coz it the highest)
data.reason_sinovac=data.reason_sinovac.replace(["."], "Success rate of the vaccine")

data.reason_sinovac=data.reason_sinovac.replace(["Taken it before, it was fine"], "Experience ")
data.reason_sinovac=data.reason_sinovac.replace(["It's from China, isn't it? "], "Influence from social media (News, Tweets, Fb sharing etc)")

data.reason_sinovac.value_counts()

"""# reason_cansino preprocess"""

data.reason_cansino.value_counts()

#handle outlier by replacing '.' with 'success rate of the vaccine' (why? coz it the highest)
data.reason_cansino=data.reason_cansino.replace(["."], "Success rate of the vaccine")

data.reason_cansino=data.reason_cansino.replace("Never heard", "not familiar").replace("never heard of it", "not familiar").replace("never heard of this vaccine name", "not familiar").replace("Im not sure about this", "not familiar").replace(["Never heard of it"], "not familiar").replace(["Not so popular "], "not familiar").replace(["didn't know about this vaccine"], "not familiar").replace(["Not know"], "not familiar").replace(["Never heard of it. "], "not familiar").replace(["Never heard before"], "not familiar").replace(["never heard of this one"], "not familiar").replace(["never heard before"], "not familiar").replace(["Rarely heard about it "], "not familiar").replace(["I didn't know about this"], "not familiar").replace(["I am not familiar with the brand"], "not familiar")

data.reason_cansino.value_counts()

#Now we can use Pandas explode() function to separate list elements into separate rows()
data=data.assign(reason_cansino=data.reason_cansino.str.split(";")).explode('reason_cansino')

data.reason_cansino.value_counts()

"""# New section"""

data.columns

data.shape

data.other_vacc.value_counts()

# Encoding values

data['gender'] = data['gender'].replace({'Male':0,'Female':1}).astype(np.uint8)
data['loc'] = data['loc'].replace({'Urban':0,'Countryside':1}).astype(np.uint8)
data['edu_background'] = data['edu_background'].replace({'None of the above':0,'Medical field (i.e. Bachelor of Medicine)':1,'Islamic field (i.e. Diploma in Islamic Studies)':2}).astype(np.uint8)
data['dose'] = data['dose'].replace({'Dose 1;Dose 2':0,'Dose 1;Dose 2;Booster 1':1,'Dose 1;Dose 2;Booster 1;Booster 2':2}).astype(np.uint8)
data['vacc1_type'] = data['vacc1_type'].replace({'Pfizer-BioNTech':0,'Sinovac':1,'AstraZeneca':2}).astype(np.uint8)
data['vacc2_type'] = data['vacc2_type'].replace({'Pfizer-BioNTech':0,'Sinovac':1,'AstraZeneca':2}).astype(np.uint8)
data['side_effect1'] = data['side_effect1'].replace({'Most common side effects (i.e. headache, injection site pain)':0,
                                                     'Common side effects (i.e. fever, vomitting)':1,
                                                     'Uncommon side effects (i.e. night sweats, heart inflammation)':2,
                                                     'None':3}).astype(np.uint8)
data['side_effect2'] = data['side_effect2'].replace({'Most common side effects (i.e. headache, injection site pain)':0,
                                                     'Common side effects (i.e. fever, vomitting)':1,
                                                     'Uncommon side effects (i.e. night sweats, heart inflammation)':2,
                                                     'None':3}).astype(np.uint8)
data["vacc_type_booster1"] = data["vacc_type_booster1"].replace({'Pfizer-BioNTech':0,'Sinovac':1,'AstraZeneca':2,"0":-1})#.astyper(np.uint8)
data["vacc_type_booster2"] = data["vacc_type_booster2"].replace({'Pfizer-BioNTech':0,'Sinovac':1,'AstraZeneca':2,"0":-1})#.astyper(np.uint8)
data["motivation"] = data["motivation"].replace({"Self awareness":0,"Social influence (Friends, Family, Peers)":1,"Government law":2,"The need to go abroad":3,"Clinical posting in the hospital ":4})#.astype(uint8)
data["family_vaccinated"] = data["family_vaccinated"].replace({"Yes, my whole family members are vaccinated":0,"No, not all of my family members are vaccinated":1})#.astyper(np.uint8)
data["lost_vaccinated"] = data["lost_vaccinated"].replace({"No":0,"Yes":1})#.astyper(np.uint8)
data["5th_vacc"] = data["5th_vacc"].replace({"No":0,"Yes":1,"Maybe":2})#.astyper(np.uint8)
data["reason_pfizer"] = data["reason_pfizer"].replace({"Success rate of the vaccine":0,
                                                       "Brand image/quality":1,
                                                       "Influence from friends/family/colleague":2,
                                                       "Influence from social media (News, Tweets, Fb sharing etc)":3,
                                                       "Always use that one":4})#.astyper(np.uint8)
data["reason_sinovac"] = data["reason_sinovac"].replace({"Success rate of the vaccine":0,
                                                       "Brand image/quality":1,
                                                       "Influence from friends/family/colleague":2,
                                                       "Influence from social media (News, Tweets, Fb sharing etc)":3,
                                                       "i just get whatever available for me as long as it doesn't cause any harm":4,
                                                       "Experience ":5})#.astyper(np.uint8)
data["reason_astrazeneca"] = data["reason_astrazeneca"].replace({"Success rate of the vaccine":0,
                                                       "Brand image/quality":1,
                                                       "Influence from friends/family/colleague":2,
                                                       "Influence from social media (News, Tweets, Fb sharing etc)":3,
                                                       "i just get whatever available for me as long as it doesn't cause any harm":4})#.astyper(np.uint8)
data["reason_cansino"] = data["reason_cansino"].replace({"Success rate of the vaccine":0,
                                                       "Brand image/quality":1,
                                                       "Influence from friends/family/colleague":2,
                                                       "Influence from social media (News, Tweets, Fb sharing etc)":3,
                                                       "not familiar":4})#.astyper(np.uint8)
data['other_vacc'] = data['other_vacc'].replace({"No":0,"Yes":1}).astype(np.uint8)

data.info()

"""# Ananlyze by Pivoting Feature"""

data[['pfizer_rate', '5th_vacc']].groupby(['pfizer_rate'], as_index=False).mean().sort_values(by='5th_vacc', ascending=False)

data[['sinovac_rate', '5th_vacc']].groupby(['sinovac_rate'], as_index=False).mean().sort_values(by='5th_vacc', ascending=False)

data[['astrazeneca_rate', '5th_vacc']].groupby(['astrazeneca_rate'], as_index=False).mean().sort_values(by='5th_vacc', ascending=False)

data[['cansino_rate', '5th_vacc']].groupby(['cansino_rate'], as_index=False).mean().sort_values(by='5th_vacc', ascending=False)

"""# Correlating features
research prob-1 (vacc brands)
"""

import seaborn as sns
import matplotlib.pyplot as plt

a = sns.FacetGrid(data, col='5th_vacc')
a.map(plt.hist, "pfizer_rate", bins=6)

b = sns.FacetGrid(data, col='5th_vacc')
b.map(plt.hist, "sinovac_rate", bins=5)

c = sns.FacetGrid(data, col='5th_vacc')
c.map(plt.hist, "astrazeneca_rate", bins=5)

"""
research prob-2(education)"""

d = sns.FacetGrid(data, col='5th_vacc')
d.map(plt.hist, "edu_background", bins=3)

"""research prob-3 motivation"""

e = sns.FacetGrid(data, col='5th_vacc')
e.map(plt.hist, "motivation", bins=6)

"""# NN"""

#Assign data from 2nd to 10th column to variable x
x=data.drop(['5th_vacc'], axis=1)

#Assign data from 'Glass type' column to varibale y
y=data['5th_vacc']

#Train test split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

#Feature scalling (only on train data not test data!)
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaler.fit(x_train)

x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

#Traning and predictions
from sklearn.pipeline import make_pipeline
from sklearn.neural_network import MLPClassifier
mlp=MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=1000)

mlp.fit(x_train, y_train.values.ravel())

#Making predictions
predictions = mlp.predict(x_test)

#Evaluating algo
from sklearn.metrics import classification_report, confusion_matrix
#print(confusion_matrix(y_test, predictions))
print(classification_report(y_test,predictions))

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
acc_nn=accuracy_score(y_test,predictions)

acc_nn=accuracy_score(y_test,predictions)
f1_nn=f1_score(y_test,predictions, average="macro")
precision_nn=precision_score(y_test,predictions, average="macro")
recall_nn=recall_score(y_test,predictions, average="macro")

corr_matrix = data.corr()
print(corr_matrix["5th_vacc"].sort_values(ascending=False))

#pip install shap

#import shap as shap

#explainer = shap.DeepExplainer(mlp, x_train)
#explainer = shap.KernelExplainer(model.predict, x_train)

# calculate shap values. This is what we will plot.
#shap_values = explainer.shap_values(x_test)

"""# Naiive Bayes"""

pip install shutup

X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.20, random_state=0)

from sklearn.naive_bayes import GaussianNB
classifier= GaussianNB()
classifier.fit(X_train,Y_train)

#Predict the test set results
Y_pred=classifier.predict(X_test)
Y_test,Y_pred

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
acc_nb=accuracy_score(Y_test,Y_pred)
#print(acc)

#Evaluating algo
#from sklearn.metrics import classification_report, confusion_matrix
#print(confusion_matrix(y_test, predictions))
print(classification_report(Y_test,Y_pred))

f1_nb=f1_score(Y_test,Y_pred, average="macro")
precision_nb=precision_score(Y_test,Y_pred, average="macro")
recall_nb=recall_score(Y_test,Y_pred, average="macro")

"""# Performance comparison"""

nn_df = pd.DataFrame(data=[f1_nn,acc_nn, recall_nn,precision_nn],
             columns=['Neural Network (NN) Score'],
             index=["F1","Accuracy", "Recall", "Precision"])

nb_df = pd.DataFrame(data=[f1_nb,acc_nb, recall_nb,precision_nb],
             columns=['Naive Bayed (NB) Score'],
             index=["F1","Accuracy", "Recall", "Precision"])

import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns

df_models = round(pd.concat([nn_df,nb_df], axis=1),3)

colors = ["lightgray","lightgray","orange"]
colormap = matplotlib.colors.LinearSegmentedColormap.from_list("", colors)

background_color = "#fbfbfb"

fig = plt.figure(figsize=(10,8)) # create figure
gs = fig.add_gridspec(4, 2)
gs.update(wspace=0.1, hspace=0.5)
ax0 = fig.add_subplot(gs[0, :])

sns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=".1%",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={"fontsize":12})
fig.patch.set_facecolor(background_color) # figure background color
ax0.set_facecolor(background_color)

ax0.text(0,-0.5,'Model Comparison',fontsize=18,fontweight='bold',fontfamily='serif')
ax0.tick_params(axis=u'both', which=u'both',length=0)
